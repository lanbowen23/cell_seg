{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import skimage.io\n",
    "import skimage.segmentation\n",
    "import skimage.morphology\n",
    "\n",
    "import sys\n",
    "__file__ = 'full_experiment.ipynb'\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "import utils.dirtools  # utils package should has __init__.py in it\n",
    "import utils.augmentation\n",
    "import utils.model_builder\n",
    "import utils.data_provider\n",
    "import utils.metrics\n",
    "import utils.objectives\n",
    "import utils.evaluation\n",
    "\n",
    "import keras.backend\n",
    "import keras.callbacks\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "from config import config_vars\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "def empty_dir(folder):\n",
    "    print('empty directory: ', folder)\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  ### \n",
    "\n",
    "# build session running on GPU 1\n",
    "configuration = tf.ConfigProto()\n",
    "configuration.gpu_options.allow_growth = True\n",
    "# configuration.gpu_options.visible_device_list = \"0, 1\"\n",
    "session = tf.Session(config = configuration)\n",
    "\n",
    "# apply session\n",
    "keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_vars[\"root_directory\"] = 'DATA/FISH/'\n",
    "experiment_name = '26'\n",
    "\n",
    "config_vars = utils.dirtools.setup_working_directories(config_vars)\n",
    "config_vars = utils.dirtools.setup_experiment(config_vars, experiment_name)\n",
    "os.makedirs(config_vars[\"normalized_images_dir\"], exist_ok=True)\n",
    "os.makedirs(config_vars[\"boundary_labels_dir\"], exist_ok=True)\n",
    "\n",
    "config_vars[\"no_boundary_labels_dir\"] = 'DATA/FISH/no_boundary_labels/'\n",
    "os.makedirs(config_vars[\"no_boundary_labels_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root_directory': 'DATA/FISH/',\n",
       " 'pixel_depth': 8,\n",
       " 'min_nucleus_size': 25,\n",
       " 'boundary_size': 2,\n",
       " 'learning_rate': 0.0001,\n",
       " 'epochs': 200,\n",
       " 'steps_per_epoch': 300,\n",
       " 'batch_size': 16,\n",
       " 'val_batch_size': 16,\n",
       " 'rescale_labels': True,\n",
       " 'crop_size': 256,\n",
       " 'cell_min_size': 16,\n",
       " 'boundary_boost_factor': 1,\n",
       " 'object_dilation': 3,\n",
       " 'raw_images_dir': 'DATA/FISH/raw_images/',\n",
       " 'raw_annotations_dir': 'DATA/FISH/raw_annotations/',\n",
       " 'normalized_images_dir': 'DATA/FISH/norm_images/',\n",
       " 'boundary_labels_dir': 'DATA/FISH/boundary_labels/',\n",
       " 'experiment_dir': 'DATA/FISH/experiments/26/out/',\n",
       " 'probmap_out_dir': 'DATA/FISH/experiments/26/out/prob/',\n",
       " 'labels_out_dir': 'DATA/FISH/experiments/26/out/segm/',\n",
       " 'path_files_training': 'DATA/FISH/experiments/26/training.txt',\n",
       " 'path_files_validation': 'DATA/FISH/experiments/26/validation.txt',\n",
       " 'path_files_test': 'DATA/FISH/experiments/26/test.txt',\n",
       " 'model_file': 'DATA/FISH/experiments/26/model.hdf5',\n",
       " 'csv_log_file': 'DATA/FISH/experiments/26/log.csv',\n",
       " 'no_boundary_labels_dir': 'DATA/FISH/no_boundary_labels/'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "'path_files_training': 'DATA/LinearTracking/training.txt',\n",
    "'path_files_validation': 'DATA/LinearTracking/validation.txt',\n",
    "'path_files_test': 'DATA/LinearTracking/test.txt',\n",
    "\n",
    "folder 001 - 095 total 1585 images, 16.68 for one folder\n",
    "setup dirs: `normalized` and `boundary label`\n",
    "img list should contain file names like 001/0000.tif\n",
    "\n",
    "\"\"\"  \n",
    "\n",
    "fd_list = sorted(os.listdir('DATA/LineageTracking/raw_images/'))\n",
    "# makedirs for 001, 002, ...\n",
    "for f in fd_list:\n",
    "    os.makedirs(config_vars[\"normalized_images_dir\"] + f, exist_ok=True)\n",
    "    os.makedirs(config_vars[\"boundary_labels_dir\"] + f, exist_ok=True)   \n",
    "\n",
    "\"\"\"split train, valid, test (image name list)\n",
    "\n",
    "\"\"\"\n",
    "train_fd_list = fd_list[:60]\n",
    "valid_fd_list = fd_list[60:]\n",
    "\n",
    "# boundary_labels_gai是清洗过后的labels\n",
    "list_train = []\n",
    "for f in train_fd_list:\n",
    "    tmp_list = os.listdir('DATA/LineageTracking/boundary_labels_gai/' + f)\n",
    "    tmp_list = [x for x in tmp_list if x.endswith('png')]\n",
    "    for e in sorted(tmp_list):\n",
    "        list_train.append(f + '/' + e)\n",
    "        \n",
    "list_valid = []\n",
    "for f in valid_fd_list:\n",
    "    tmp_list = os.listdir('DATA/LineageTracking/boundary_labels/' + f)\n",
    "    tmp_list = [x for x in tmp_list if x.endswith('png')]\n",
    "    for e in sorted(tmp_list):\n",
    "        list_valid.append(f + '/' + e)\n",
    "        \n",
    "list_test = []\n",
    "\n",
    "\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_training\"], list_train)\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_validation\"], list_valid)\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_test\"], list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = os.listdir(config_vars[\"normalized_images_dir\"])\n",
    "# image_list = [x for x in file_list if x.endswith(\"png\")]\n",
    "# print(\"total images: {}\".format(len(image_list)))\n",
    "\n",
    "# set up train-valid split EVERY-TIME\n",
    "def create_image_lists(dir_raw_images):\n",
    "    file_list = os.listdir(dir_raw_images)\n",
    "    image_list = [x for x in file_list if x.endswith(\"png\")]\n",
    "    image_list = sorted(image_list)\n",
    "\n",
    "    image_list_train_aug = []\n",
    "    image_list_test = []\n",
    "#     image_list_train = []\n",
    "#     image_list_validation = image_list\n",
    "    \n",
    "    image_list_validation = image_list[:48]\n",
    "    image_list_2 = image_list[48:]\n",
    "    random.shuffle(image_list_2)\n",
    "    image_list_train = image_list_2\n",
    "    return image_list_train, image_list_test, image_list_validation, image_list_train_aug\n",
    "\n",
    "[list_training, list_test, list_validation, list_training_aug] = create_image_lists(\n",
    "    config_vars[\"normalized_images_dir\"],\n",
    "#         config_vars[\"training_fraction\"],\n",
    "#         config_vars[\"validation_fraction\"]\n",
    ")\n",
    "\n",
    "# write list into txt file\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_training\"], list_training)\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_validation\"], list_validation)\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_test\"], list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file into dict partitions with 3 list for train/valid/test\n",
    "data_partitions = utils.dirtools.read_data_partitions(config_vars, load_augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data-generator\n",
    "train_gen = utils.data_provider.random_sample_generator(\n",
    "    config_vars[\"normalized_images_dir\"],\n",
    "    config_vars[\"boundary_labels_dir\"],  ### boundary_labels_dir no_boundary_labels_dir\n",
    "    data_partitions[\"training\"],\n",
    "    config_vars[\"batch_size\"],\n",
    "    config_vars[\"pixel_depth\"],\n",
    "    config_vars[\"crop_size\"],\n",
    "    config_vars[\"crop_size\"],\n",
    "    config_vars[\"rescale_labels\"]\n",
    ")\n",
    "\n",
    "val_gen = utils.data_provider.single_data_from_images(\n",
    "     config_vars[\"normalized_images_dir\"],\n",
    "     config_vars[\"boundary_labels_dir\"],  ### boundary_labels_dir no_boundary_labels_dir\n",
    "     data_partitions[\"validation\"],\n",
    "     config_vars[\"val_batch_size\"],\n",
    "     config_vars[\"pixel_depth\"],\n",
    "     config_vars[\"crop_size\"],\n",
    "     config_vars[\"crop_size\"],\n",
    "     config_vars[\"rescale_labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 291 images.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(train_gen)\n",
    "a = next(train_iter)\n",
    "len(a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traininig Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "\n",
    "# delete boundary, output_channel=1, activation=\"sigmoid\"\n",
    "# with boundary, output_channel=3, activation=None\n",
    "model = utils.model_builder.get_model(config_vars[\"crop_size\"], config_vars[\"crop_size\"], \n",
    "                                      output_channel=3, activation=None) \n",
    "# softmax in the end match with crossentropy\n",
    "# use sigmoid for one class?\n",
    "# loss = \"binary_crossentropy\"\n",
    "loss = utils.objectives.weighted_crossentropy\n",
    "\n",
    "### PRINT RESULT In the TRAIN ###\n",
    "# def tf_print(op, tensors, message=None):\n",
    "#     def print_message(x):\n",
    "#         sys.stdout.write(message + \" %s\\n\" % x)\n",
    "#         return x\n",
    "#     prints = [tf.py_func(print_message, [tensor], tensor.dtype) for tensor in tensors]\n",
    "#     with tf.control_dependencies(prints):\n",
    "#         op = tf.identity(op)\n",
    "#     return op\n",
    "\n",
    "# def my_loss(y_true, y_pred):\n",
    "#     loss = -tf.reduce_sum(y_true * tf.log(y_pred))\n",
    "#     loss = tf_print(y_pred, [tf.reduce_max(y_pred)], 'y_pred max = ')\n",
    "#     return loss\n",
    "### PRINT RESULT In the TRAIN ###\n",
    "\n",
    "my_metrics = [\n",
    "           keras.metrics.categorical_accuracy, \n",
    "           utils.metrics.channel_recall(channel=0, name=\"background_recall\"), \n",
    "           utils.metrics.channel_precision(channel=0, name=\"background_precision\"),\n",
    "           utils.metrics.channel_recall(channel=1, name=\"interior_recall\"), \n",
    "           utils.metrics.channel_precision(channel=1, name=\"interior_precision\"),\n",
    "           utils.metrics.channel_recall(channel=2, name=\"boundary_recall\"), \n",
    "           utils.metrics.channel_precision(channel=2, name=\"boundary_precision\"),\n",
    "          ]\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=config_vars[\"learning_rate\"])\n",
    "\n",
    "###\n",
    "# model.compile(loss=loss, metrics=[metrics.binary_accuracy], optimizer=optimizer)\n",
    "\n",
    "model.compile(loss=loss, metrics=my_metrics, optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "log_folder = 'logs/'\n",
    "\n",
    "csv = keras.callbacks.CSVLogger(filename=config_vars[\"csv_log_file\"])  # append\n",
    "\n",
    "tboard = keras.callbacks.TensorBoard(log_dir=log_folder + experiment_name, \n",
    "                                      histogram_freq=0, \n",
    "                                      batch_size=16, \n",
    "                                      write_graph=True, \n",
    "                                      write_grads=False, write_images=True,\n",
    "                                      update_freq='epoch')\n",
    "# add ModelCheckpoints\n",
    "# monitor val-loss\n",
    "weights_filename1 = log_folder + experiment_name + '/model-{epoch:02d}-{val_loss:.2f}.h5'\n",
    "modelckp1 = keras.callbacks.ModelCheckpoint(weights_filename1, verbose=1, save_weights_only=True,\n",
    "                                           monitor='val_loss', period=1, save_best_only=True)\n",
    "weights_filename2 = log_folder + experiment_name + '/model-{epoch:02d}-{loss:.2f}.h5'\n",
    "modelckp2 = keras.callbacks.ModelCheckpoint(weights_filename2, verbose=1, save_weights_only=True,\n",
    "                                           monitor='loss', period=1, save_best_only=True)\n",
    "\n",
    "# min_delta: threshold for measuring the new optimum,\n",
    "#       to only focus on significant changes.\n",
    "# cooldown: number of epochs to wait before resuming\n",
    "#       normal operation after lr has been reduced.\n",
    "reducelr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, \n",
    "                                             verbose=1, mode='min', min_lr=1e-7, \n",
    "                                             cooldown=10, min_delta=1e-4)\n",
    "# min_lr could be smaller\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=15, \n",
    "                              verbose=1, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "callbacks = [csv, tboard, modelckp1, modelckp2, reducelr, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Training with 291 images.\n",
      " 41/300 [===>..........................] - ETA: 3:26 - loss: 0.9037 - categorical_accuracy: 0.8586 - background_recall: 0.8581 - background_precision: 0.9891 - interior_recall: 0.8296 - interior_precision: 0.9262 - boundary_recall: 0.9585 - boundary_precision: 0.3655"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "statistics = model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    steps_per_epoch=config_vars[\"steps_per_epoch\"],  # 500 config_vars[\"steps_per_epoch\"]\n",
    "    epochs=config_vars[\"epochs\"],\n",
    "    validation_data=val_gen,\n",
    "    validation_steps= int(config_vars[\"steps_per_epoch\"]/6),  # must bigger than val_batch_size\n",
    "    callbacks=callbacks,\n",
    "    verbose = 1\n",
    ")\n",
    "print('Done! :)')\n",
    "\n",
    "# save one weight at the end of the training\n",
    "model.save_weights(config_vars[\"model_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [os.path.join(config_vars[\"normalized_images_dir\"], f) for f in data_partitions[\"validation\"]]\n",
    "imagebuffer = skimage.io.imread_collection(image_names)\n",
    "images = imagebuffer.concatenate()\n",
    "\n",
    "dim1, dim2 = images.shape[1], images.shape[2]\n",
    "images = images.reshape((-1, dim1, dim2, 1))\n",
    "# preprocess (assuming images are encoded as 8-bits in the preprocessing step)\n",
    "images = images / 255\n",
    "\n",
    "### build model and load weights\n",
    "# model = utils.model_builder.get_model(dim1, dim2, output_channel=1, activation=\"sigmoid\") # stupid error!\n",
    "model = utils.model_builder.get_model(dim1, dim2, output_channel=3, activation=None)\n",
    "\n",
    "model.load_weights(config_vars[\"model_file\"])\n",
    "# model.load_weights('logs/15/model-01-0.19.h5')\n",
    "\n",
    "predictions = model.predict(images, batch_size=1)\n",
    "\n",
    "\"\"\"prepare gt annot & bd image names\n",
    "\n",
    "rl: abrev. for raw label\n",
    "bl: abrev. for boundary label\n",
    "\"\"\"\n",
    "rl_names = [os.path.join(config_vars[\"raw_annotations_dir\"], f) for f in data_partitions[\"validation\"]]\n",
    "rl_buffer = skimage.io.imread_collection(rl_names) \n",
    "bl_names = [os.path.join(config_vars[\"boundary_labels_dir\"], f) for f in data_partitions[\"validation\"]]\n",
    "bl_buffer = skimage.io.imread_collection(bl_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty directory:  DATA/FISH/experiments/26/out/prob/\n",
      "empty directory:  DATA/FISH/experiments/26/out/segm/\n"
     ]
    }
   ],
   "source": [
    "empty_dir(config_vars[\"probmap_out_dir\"])\n",
    "empty_dir(config_vars[\"labels_out_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(images)):\n",
    "    filename = imagebuffer.files[i]\n",
    "    imgname = os.path.basename(filename)\n",
    "    \n",
    "    original_image = skimage.io.imread(filename)\n",
    "    rl_image = skimage.io.imread(rl_buffer.files[i])\n",
    "    bl_image = skimage.io.imread(bl_buffer.files[i])\n",
    "    \n",
    "    probmap = predictions[i].squeeze()\n",
    "    os.makedirs(config_vars[\"probmap_out_dir\"], exist_ok=True)\n",
    "    skimage.io.imsave(config_vars[\"probmap_out_dir\"] + imgname, probmap.astype('uint8'))\n",
    "    \n",
    "#     pred = probmap.copy()\n",
    "#     thre = 0.5\n",
    "#     pred[probmap >= thre] = 1\n",
    "#     pred[probmap < thre] = 0\n",
    "#     pred = pred.astype('uint8')\n",
    "#     cell = skimage.morphology.remove_small_holes(pred, min_size=config_vars[\"cell_min_size\"])\n",
    "#     cell = skimage.morphology.remove_small_objects(cell, min_size=config_vars[\"cell_min_size\"])\n",
    "#     [label, num] = skimage.morphology.label(cell, return_num=True)\n",
    "    \n",
    "    pred = utils.metrics.probmap_to_pred(probmap, config_vars[\"boundary_boost_factor\"])\n",
    "    label = utils.metrics.pred_to_label(pred, config_vars[\"cell_min_size\"])\n",
    "    \n",
    "    os.makedirs(config_vars[\"labels_out_dir\"], exist_ok=True)\n",
    "    skimage.io.imsave(config_vars[\"labels_out_dir\"] + imgname, label.astype('uint8'))\n",
    "    \n",
    "    if (i < 15):\n",
    "        f, ax = plt.subplots(2,3,figsize=(18,12))\n",
    "        ax[0][0].imshow(original_image)\n",
    "        ax[0][0].title.set_text('original image')\n",
    "        ax[0][1].imshow(bl_image)\n",
    "        ax[0][1].title.set_text('ground truth boundary')\n",
    "        ax[0][2].imshow(rl_image)\n",
    "        ax[0][2].title.set_text('ground truth label')\n",
    "        ax[1][0].imshow(pred)\n",
    "        ax[1][0].title.set_text('predict boundary')\n",
    "        ax[1][1].imshow(probmap)\n",
    "        ax[1][1].title.set_text('predict boundary probmap')\n",
    "        ax[1][2].imshow(label)\n",
    "        ax[1][2].title.set_text('predict label')\n",
    "        for a in ax:\n",
    "            for a_ in a:\n",
    "                a_.set_xticks([])\n",
    "                a_.set_yticks([])\n",
    "        plt.show()\n",
    "        \n",
    "#     if (i == 1):\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"this is for Lineage Tracking data with second order directories\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i in range(len(images)):\n",
    "    filename = imagebuffer.files[i]\n",
    "    # imgname = os.path.basename(filename)\n",
    "    filename_split = filename.split('/')\n",
    "    imgname = '/' + filename_split[-2] + '/' + filename_split[-1]\n",
    "    \n",
    "    original_image = skimage.io.imread(filename)\n",
    "    rl_image = skimage.io.imread(rl_buffer.files[i])\n",
    "    bl_image = skimage.io.imread(bl_buffer.files[i])\n",
    "    \n",
    "    probmap = predictions[i].squeeze()\n",
    "    os.makedirs(os.path.join(config_vars[\"probmap_out_dir\"], filename_split[-2]), exist_ok=True)\n",
    "    skimage.io.imsave(config_vars[\"probmap_out_dir\"] + imgname, probmap.astype('uint8'))\n",
    "    \n",
    "    pred = utils.metrics.probmap_to_pred(probmap, config_vars[\"boundary_boost_factor\"])\n",
    "    label = utils.metrics.pred_to_label(pred, config_vars[\"cell_min_size\"])\n",
    "    os.makedirs(os.path.join(config_vars[\"labels_out_dir\"], filename_split[-2]), exist_ok=True)\n",
    "    skimage.io.imsave(config_vars[\"labels_out_dir\"] + imgname, label.astype('uint8'))\n",
    "    \n",
    "    if (i < 15):\n",
    "        f, ax = plt.subplots(2,3,figsize=(12,8))\n",
    "        ax[0][0].imshow(original_image)\n",
    "        ax[0][0].title.set_text('original image')\n",
    "        ax[0][1].imshow(bl_image)\n",
    "        ax[0][1].title.set_text('ground truth boundary')\n",
    "        ax[0][2].imshow(rl_image)\n",
    "        ax[0][2].title.set_text('ground truth label')\n",
    "        ax[1][0].imshow(pred)\n",
    "        ax[1][0].title.set_text('predict boundary')\n",
    "        ax[1][1].imshow(probmap)\n",
    "        ax[1][1].title.set_text('predict boundary probmap')\n",
    "        ax[1][2].imshow(label)\n",
    "        ax[1][2].title.set_text('predict label')\n",
    "        for a in ax:\n",
    "            for a_ in a:\n",
    "                a_.set_xticks([])\n",
    "                a_.set_yticks([])\n",
    "        plt.show()\n",
    "        \n",
    "#     if (i == 15):\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([12, 15, 18, 21])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "understand sum operation\n",
    "\"\"\"\n",
    "\n",
    "a = np.arange(12).reshape(3,4)\n",
    "print(a[0])\n",
    "np.sum(a, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRINT Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dirs = ['DATA/DNA_FISH/norm_images/',\n",
    "              'DATA/IF_images/norm_images/',\n",
    "              'DATA/Lineage_Tracking_ZY/norm_images/',\n",
    "              'DATA/Lineage_Tracking_KZ/norm_images/']\n",
    "\n",
    "labels_dirs = ['DATA/DNA_FISH/experiments/04/out/segm/',\n",
    "              'DATA/IF_images/experiments/04/out/segm/',\n",
    "              'DATA/Lineage_Tracking_ZY/experiments/04/out/segm/',\n",
    "              'DATA/Lineage_Tracking_KZ/experiments/04/out/segm/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(raw_label, pred_label):\n",
    "    ground_truth = raw_label.copy()\n",
    "    prediction = pred_label.copy()\n",
    "    IOU = utils.evaluation.intersection_over_union(ground_truth, prediction)\n",
    "    diff = np.zeros(ground_truth.shape + (3,))  # become 3 channels\n",
    "    A = ground_truth.copy()\n",
    "    B = prediction.copy()\n",
    "    A[A > 0] = 1\n",
    "    B[B > 0] = 1\n",
    "    D = A - B\n",
    "    #diff[D > 0,:2] = 1\n",
    "    #diff[D < 0,1:] = 1\n",
    "    \n",
    "    # Object-level errors\n",
    "    C = IOU.copy()\n",
    "    threshold = 0.5  # if set to 0.8, more misses will appear, but 0.5 no miss\n",
    "    C[C >= threshold] = 1\n",
    "    C[C < threshold] = 0\n",
    "    missed = np.where(np.sum(C, axis = 1) == 0)[0]  # for original cell, none predict cell match \n",
    "    extra = np.where(np.sum(C, axis = 0) == 0)[0]  # for predict cell, none original cell match\n",
    "\n",
    "    for m in missed:\n",
    "        diff[ground_truth == m + 1, 0] = 1\n",
    "    for e in extra:\n",
    "        diff[prediction == e + 1, 2] = 1\n",
    "        \n",
    "    return diff, str(len(missed)), str(len(extra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(img_name):\n",
    "    original_images = []\n",
    "    pred_labels = []\n",
    "    for i in range(4):\n",
    "        ori_img_filename = os.path.join(images_dirs[i], img_name)\n",
    "        original_image = skimage.io.imread(ori_img_filename)\n",
    "    \n",
    "        pred_label_filename = os.path.join(labels_dirs[i], img_name)\n",
    "        pred_label = skimage.io.imread(pred_label_filename)\n",
    "        \n",
    "        struct = skimage.morphology.square(3)\n",
    "        pred_label = skimage.morphology.dilation(pred_label, struct)\n",
    "        pred_label = skimage.segmentation.relabel_sequential(pred_label)[0] #[30:-30,30:-30])[0]\n",
    "            \n",
    "        # make graph easier to look\n",
    "        inc = lambda x: x if x == 0 else x + 100\n",
    "        inc = np.vectorize(inc)\n",
    "        pred_label = inc(pred_label)\n",
    "        \n",
    "        original_images.append(original_image)\n",
    "        pred_labels.append(pred_label)\n",
    "        \n",
    "    fig, ax = plt.subplots(2, 4, figsize=(20,10))\n",
    "    fig.suptitle(img_name)\n",
    "    ax[0][0].set_title(\"DNA_FISH\")\n",
    "    ax[0][0].imshow(original_images[0])\n",
    "    ax[0][1].set_title(\"IF_images\")\n",
    "    ax[0][1].imshow(original_images[1])\n",
    "    ax[0][2].set_title(\"Linear_Tracking_zy\")\n",
    "    ax[0][2].imshow(original_images[2])\n",
    "    ax[0][3].set_title(\"Linear_Tracking_kz\")\n",
    "    ax[0][3].imshow(original_images[3])\n",
    "    \n",
    "#     ax[1][0].set_title(\"DNA_FISH\")\n",
    "    ax[1][0].imshow(pred_labels[0])\n",
    "#     ax[1][1].set_title(\"IF_images\")\n",
    "    ax[1][1].imshow(pred_labels[1])\n",
    "#     ax[1][2].set_title(\"Linear_Tracking_zy\")\n",
    "    ax[1][2].imshow(pred_labels[2])\n",
    "#     ax[1][3].set_title(\"Linear_Tracking_kz\")\n",
    "    ax[1][3].imshow(pred_labels[3])\n",
    "\n",
    "\n",
    "    # plt.figure(figsize=(6,6))\n",
    "    # plt.imshow(original_image)  #, cmap=\"nipy_spectral\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.randint(74, size = 20):\n",
    "    img = \"{:04}\".format(i) + \".png\"\n",
    "    compare(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
