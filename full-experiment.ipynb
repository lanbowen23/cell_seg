{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import skimage.io\n",
    "import skimage.segmentation\n",
    "import skimage.morphology\n",
    "\n",
    "import sys\n",
    "__file__ = 'full_experiment.ipynb'\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "import utils.dirtools  # utils package should has __init__.py in it\n",
    "import utils.augmentation\n",
    "import utils.model_builder\n",
    "import utils.data_provider\n",
    "import utils.metrics\n",
    "import utils.objectives\n",
    "import utils.evaluation\n",
    "\n",
    "import keras.backend\n",
    "import keras.callbacks\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "from config import config_vars\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "def empty_dir(folder):\n",
    "    print('empty directory: ', folder)\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  ### \n",
    "\n",
    "# build session running on GPU 1\n",
    "configuration = tf.ConfigProto()\n",
    "configuration.gpu_options.allow_growth = True\n",
    "# configuration.gpu_options.visible_device_list = \"0, 1\"\n",
    "session = tf.Session(config = configuration)\n",
    "\n",
    "# apply session\n",
    "keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup global variables\n",
    "\n",
    "config_vars[\"root_directory\"] = 'DATA/FISH/'\n",
    "experiment_name = '26'\n",
    "\n",
    "config_vars = utils.dirtools.setup_working_directories(config_vars)\n",
    "config_vars = utils.dirtools.setup_experiment(config_vars, experiment_name)\n",
    "os.makedirs(config_vars[\"normalized_images_dir\"], exist_ok=True)\n",
    "os.makedirs(config_vars[\"boundary_labels_dir\"], exist_ok=True)\n",
    "\n",
    "config_vars[\"no_boundary_labels_dir\"] = 'DATA/FISH/no_boundary_labels/'\n",
    "os.makedirs(config_vars[\"no_boundary_labels_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Data Split\n",
    "do this everytime before training or prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For Lineage_Tracking Datasets\n",
    "save train/val/test image pathnames into txt files\n",
    "\n",
    "'path_files_training': 'DATA/LinearTracking/training.txt',\n",
    "'path_files_validation': 'DATA/LinearTracking/validation.txt',\n",
    "'path_files_test': 'DATA/LinearTracking/test.txt',\n",
    "\n",
    "folder 001 - 095 total 1585 images, 16.68 for one folder\n",
    "setup dirs: `normalized` and `boundary label`\n",
    "img list should contain file names like 001/0000.tif\n",
    "\n",
    "\"\"\"  \n",
    "\n",
    "fd_list = sorted(os.listdir('DATA/LineageTracking/raw_images/'))\n",
    "# makedirs for 001, 002, ...\n",
    "for f in fd_list:\n",
    "    os.makedirs(config_vars[\"normalized_images_dir\"] + f, exist_ok=True)\n",
    "    os.makedirs(config_vars[\"boundary_labels_dir\"] + f, exist_ok=True)   \n",
    "\n",
    "\"\"\"split train, valid, test (image name list)\n",
    "\n",
    "\"\"\"\n",
    "train_fd_list = fd_list[:60]\n",
    "valid_fd_list = fd_list[60:]\n",
    "\n",
    "list_train = []\n",
    "for f in train_fd_list:\n",
    "    tmp_list = os.listdir('DATA/LineageTracking/boundary_labels/' + f)\n",
    "    tmp_list = [x for x in tmp_list if x.endswith('png')]\n",
    "    for e in sorted(tmp_list):\n",
    "        list_train.append(f + '/' + e)\n",
    "        \n",
    "list_valid = []\n",
    "for f in valid_fd_list:\n",
    "    tmp_list = os.listdir('DATA/LineageTracking/boundary_labels/' + f)\n",
    "    tmp_list = [x for x in tmp_list if x.endswith('png')]\n",
    "    for e in sorted(tmp_list):\n",
    "        list_valid.append(f + '/' + e)\n",
    "        \n",
    "list_test = []\n",
    "\n",
    "\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_training\"], list_train)\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_validation\"], list_valid)\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_test\"], list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For FISH Datasets\n",
    "save train/val/test image pathnames into txt files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# set up train-valid split EVERY-TIME\n",
    "def create_image_lists(dir_raw_images):\n",
    "    file_list = os.listdir(dir_raw_images)\n",
    "    image_list = [x for x in file_list if x.endswith(\"png\")]\n",
    "    image_list = sorted(image_list)\n",
    "\n",
    "    image_list_train_aug = []\n",
    "    image_list_test = []\n",
    "#     image_list_train = []\n",
    "#     image_list_validation = image_list\n",
    "    \n",
    "    image_list_validation = image_list[:48]\n",
    "    image_list_2 = image_list[48:]\n",
    "    random.shuffle(image_list_2)\n",
    "    image_list_train = image_list_2\n",
    "    return image_list_train, image_list_test, image_list_validation, image_list_train_aug\n",
    "\n",
    "[list_training, list_test, list_validation, list_training_aug] = create_image_lists(\n",
    "    config_vars[\"normalized_images_dir\"],\n",
    ")\n",
    "\n",
    "# write list into txt file\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_training\"], list_training)\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_validation\"], list_validation)\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_test\"], list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"read split filenames txt files into data_partitions dictionary\"\"\"\n",
    "\n",
    "data_partitions = utils.dirtools.read_data_partitions(config_vars, load_augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data-generator\n",
    "train_gen = utils.data_provider.random_sample_generator(\n",
    "    config_vars[\"normalized_images_dir\"],\n",
    "    config_vars[\"boundary_labels_dir\"],  ### boundary_labels_dir no_boundary_labels_dir\n",
    "    data_partitions[\"training\"],\n",
    "    config_vars[\"batch_size\"],\n",
    "    config_vars[\"pixel_depth\"],\n",
    "    config_vars[\"crop_size\"],\n",
    "    config_vars[\"crop_size\"],\n",
    "    config_vars[\"rescale_labels\"]\n",
    ")\n",
    "\n",
    "val_gen = utils.data_provider.single_data_from_images(\n",
    "     config_vars[\"normalized_images_dir\"],\n",
    "     config_vars[\"boundary_labels_dir\"],  ### boundary_labels_dir no_boundary_labels_dir\n",
    "     data_partitions[\"validation\"],\n",
    "     config_vars[\"val_batch_size\"],\n",
    "     config_vars[\"pixel_depth\"],\n",
    "     config_vars[\"crop_size\"],\n",
    "     config_vars[\"crop_size\"],\n",
    "     config_vars[\"rescale_labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traininig Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "\n",
    "# delete boundary, one binary channel, output_channel=1, activation=\"sigmoid\"\n",
    "# with boundary, three binary channels, output_channel=3, activation=None\n",
    "model = utils.model_builder.get_model(config_vars[\"crop_size\"], config_vars[\"crop_size\"], \n",
    "                                      output_channel=3, activation=None) \n",
    "\n",
    "# loss = \"binary_crossentropy\"\n",
    "loss = utils.objectives.weighted_crossentropy\n",
    "\n",
    "my_metrics = [\n",
    "           keras.metrics.categorical_accuracy, \n",
    "           utils.metrics.channel_recall(channel=0, name=\"background_recall\"), \n",
    "           utils.metrics.channel_precision(channel=0, name=\"background_precision\"),\n",
    "           utils.metrics.channel_recall(channel=1, name=\"interior_recall\"), \n",
    "           utils.metrics.channel_precision(channel=1, name=\"interior_precision\"),\n",
    "           utils.metrics.channel_recall(channel=2, name=\"boundary_recall\"), \n",
    "           utils.metrics.channel_precision(channel=2, name=\"boundary_precision\"),\n",
    "          ]\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=config_vars[\"learning_rate\"])\n",
    "\n",
    "###\n",
    "# model.compile(loss=loss, metrics=[metrics.binary_accuracy], optimizer=optimizer)\n",
    "model.compile(loss=loss, metrics=my_metrics, optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Callbacks\n",
    "log_folder = 'logs/'\n",
    "\n",
    "csv = keras.callbacks.CSVLogger(filename=config_vars[\"csv_log_file\"])  # append\n",
    "\n",
    "tboard = keras.callbacks.TensorBoard(log_dir=log_folder + experiment_name, \n",
    "                                      histogram_freq=0, \n",
    "                                      batch_size=16, \n",
    "                                      write_graph=True, \n",
    "                                      write_grads=False, write_images=True,\n",
    "                                      update_freq='epoch')\n",
    "\n",
    "\n",
    "weights_filename1 = log_folder + experiment_name + '/model-{epoch:02d}-{val_loss:.2f}.h5'\n",
    "modelckp1 = keras.callbacks.ModelCheckpoint(weights_filename1, verbose=1, save_weights_only=True,\n",
    "                                           monitor='val_loss', period=1, save_best_only=True)\n",
    "weights_filename2 = log_folder + experiment_name + '/model-{epoch:02d}-{loss:.2f}.h5'\n",
    "modelckp2 = keras.callbacks.ModelCheckpoint(weights_filename2, verbose=1, save_weights_only=True,\n",
    "                                           monitor='loss', period=1, save_best_only=True)\n",
    "\n",
    "reducelr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, \n",
    "                                             verbose=1, mode='min', min_lr=1e-7, \n",
    "                                             cooldown=10, min_delta=1e-4)\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=15, \n",
    "                              verbose=1, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "callbacks = [csv, tboard, modelckp1, modelckp2, reducelr, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "statistics = model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    steps_per_epoch=config_vars[\"steps_per_epoch\"],  # 500 config_vars[\"steps_per_epoch\"]\n",
    "    epochs=config_vars[\"epochs\"],\n",
    "    validation_data=val_gen,\n",
    "    validation_steps= int(config_vars[\"steps_per_epoch\"]/6),  # must bigger than val_batch_size\n",
    "    callbacks=callbacks,\n",
    "    verbose = 1\n",
    ")\n",
    "print('Done! :)')\n",
    "\n",
    "# save one weight at the end of the training\n",
    "model.save_weights(config_vars[\"model_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [os.path.join(config_vars[\"normalized_images_dir\"], f) for f in data_partitions[\"validation\"]]\n",
    "imagebuffer = skimage.io.imread_collection(image_names)\n",
    "images = imagebuffer.concatenate()\n",
    "\n",
    "dim1, dim2 = images.shape[1], images.shape[2]\n",
    "images = images.reshape((-1, dim1, dim2, 1))\n",
    "# preprocess (assuming images are encoded as 8-bits in the preprocessing step)\n",
    "images = images / 255\n",
    "\n",
    "### build model and load weights\n",
    "# model = utils.model_builder.get_model(dim1, dim2, output_channel=1, activation=\"sigmoid\")\n",
    "model = utils.model_builder.get_model(dim1, dim2, output_channel=3, activation=None)\n",
    "\n",
    "model.load_weights(config_vars[\"model_file\"])\n",
    "# model.load_weights('logs/15/model-01-0.19.h5')\n",
    "\n",
    "predictions = model.predict(images, batch_size=1)\n",
    "\n",
    "\"\"\"prepare gt annot & bd image names\n",
    "\n",
    "rl: abrev. for raw label\n",
    "bl: abrev. for boundary label\n",
    "\"\"\"\n",
    "rl_names = [os.path.join(config_vars[\"raw_annotations_dir\"], f) for f in data_partitions[\"validation\"]]\n",
    "rl_buffer = skimage.io.imread_collection(rl_names) \n",
    "bl_names = [os.path.join(config_vars[\"boundary_labels_dir\"], f) for f in data_partitions[\"validation\"]]\n",
    "bl_buffer = skimage.io.imread_collection(bl_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_dir(config_vars[\"probmap_out_dir\"])\n",
    "empty_dir(config_vars[\"labels_out_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(images)):\n",
    "    filename = imagebuffer.files[i]\n",
    "    imgname = os.path.basename(filename)\n",
    "    \n",
    "    original_image = skimage.io.imread(filename)\n",
    "    rl_image = skimage.io.imread(rl_buffer.files[i])\n",
    "    bl_image = skimage.io.imread(bl_buffer.files[i])\n",
    "    \n",
    "    probmap = predictions[i].squeeze()\n",
    "    os.makedirs(config_vars[\"probmap_out_dir\"], exist_ok=True)\n",
    "    skimage.io.imsave(config_vars[\"probmap_out_dir\"] + imgname, probmap.astype('uint8'))\n",
    "    \n",
    "    pred = utils.metrics.probmap_to_pred(probmap, config_vars[\"boundary_boost_factor\"])\n",
    "    label = utils.metrics.pred_to_label(pred, config_vars[\"cell_min_size\"])\n",
    "    \n",
    "    os.makedirs(config_vars[\"labels_out_dir\"], exist_ok=True)\n",
    "    skimage.io.imsave(config_vars[\"labels_out_dir\"] + imgname, label.astype('uint8'))\n",
    "    \n",
    "    if (i < 15):\n",
    "        f, ax = plt.subplots(2,3,figsize=(18,12))\n",
    "        ax[0][0].imshow(original_image)\n",
    "        ax[0][0].title.set_text('original image')\n",
    "        ax[0][1].imshow(bl_image)\n",
    "        ax[0][1].title.set_text('ground truth boundary')\n",
    "        ax[0][2].imshow(rl_image)\n",
    "        ax[0][2].title.set_text('ground truth label')\n",
    "        ax[1][0].imshow(pred)\n",
    "        ax[1][0].title.set_text('predict boundary')\n",
    "        ax[1][1].imshow(probmap)\n",
    "        ax[1][1].title.set_text('predict boundary probmap')\n",
    "        ax[1][2].imshow(label)\n",
    "        ax[1][2].title.set_text('predict label')\n",
    "        for a in ax:\n",
    "            for a_ in a:\n",
    "                a_.set_xticks([])\n",
    "                a_.set_yticks([])\n",
    "        plt.show()\n",
    "        \n",
    "#     if (i == 1):\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"For Lineage Tracking data with second order directories\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i in range(len(images)):\n",
    "    filename = imagebuffer.files[i]\n",
    "    # imgname = os.path.basename(filename)\n",
    "    filename_split = filename.split('/')\n",
    "    imgname = '/' + filename_split[-2] + '/' + filename_split[-1]\n",
    "    \n",
    "    original_image = skimage.io.imread(filename)\n",
    "    rl_image = skimage.io.imread(rl_buffer.files[i])\n",
    "    bl_image = skimage.io.imread(bl_buffer.files[i])\n",
    "    \n",
    "    probmap = predictions[i].squeeze()\n",
    "    os.makedirs(os.path.join(config_vars[\"probmap_out_dir\"], filename_split[-2]), exist_ok=True)\n",
    "    skimage.io.imsave(config_vars[\"probmap_out_dir\"] + imgname, probmap.astype('uint8'))\n",
    "    \n",
    "    pred = utils.metrics.probmap_to_pred(probmap, config_vars[\"boundary_boost_factor\"])\n",
    "    label = utils.metrics.pred_to_label(pred, config_vars[\"cell_min_size\"])\n",
    "    os.makedirs(os.path.join(config_vars[\"labels_out_dir\"], filename_split[-2]), exist_ok=True)\n",
    "    skimage.io.imsave(config_vars[\"labels_out_dir\"] + imgname, label.astype('uint8'))\n",
    "    \n",
    "    if (i < 15):\n",
    "        f, ax = plt.subplots(2,3,figsize=(12,8))\n",
    "        ax[0][0].imshow(original_image)\n",
    "        ax[0][0].title.set_text('original image')\n",
    "        ax[0][1].imshow(bl_image)\n",
    "        ax[0][1].title.set_text('ground truth boundary')\n",
    "        ax[0][2].imshow(rl_image)\n",
    "        ax[0][2].title.set_text('ground truth label')\n",
    "        ax[1][0].imshow(pred)\n",
    "        ax[1][0].title.set_text('predict boundary')\n",
    "        ax[1][1].imshow(probmap)\n",
    "        ax[1][1].title.set_text('predict boundary probmap')\n",
    "        ax[1][2].imshow(label)\n",
    "        ax[1][2].title.set_text('predict label')\n",
    "        for a in ax:\n",
    "            for a_ in a:\n",
    "                a_.set_xticks([])\n",
    "                a_.set_yticks([])\n",
    "        plt.show()\n",
    "        \n",
    "#     if (i == 15):\n",
    "#         break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
